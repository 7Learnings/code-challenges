{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery\n",
    "%load_ext tensorboard\n",
    "import os\n",
    "import pandas as pd\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"gcloud-credentials.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Will it snow tomorrow?\" - The time traveler asked\n",
    "The following dataset contains climate information form over 9000 stations accross the world. The overall goal of these subtasks will be to predict whether it will snow tomorrow 12 years ago. So if today is 2021.02.15 then the weather we want to forecast is for the date 2009.02.16. You are suppsed to solve the tasks using Big Query, which can be used in the Jupyter Notebook like it is shown in the following cell. For further information and how to used BigQuery in Jupyter Notebook refer to the Google Docs. \n",
    "\n",
    "The goal of this test is, to test your coding knowledge in Python, BigQuery and Pandas as well as your understanding of Data Science. If you get stuck at the first part, you can use the replacement data provided in the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "*,\n",
    "FROM `bigquery-public-data.samples.gsod`\n",
    "LIMIT 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Task\n",
    "Change the date format to 'YYYY-MM-DD' and select the data from 2005 till 2009 for station numbers including and between 725300 and 726300 , and save it as a pandas dataframe. Note the maximum year available is 2010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bigquery weather_data_raw\n",
    "SELECT\n",
    "    station_number,\n",
    "    mean_temp,\n",
    "    num_mean_temp_samples,\n",
    "    mean_dew_point,\n",
    "    num_mean_dew_point_samples,\n",
    "    mean_sealevel_pressure,\n",
    "    num_mean_sealevel_pressure_samples,\n",
    "    mean_station_pressure,\n",
    "    num_mean_station_pressure_samples,\n",
    "    mean_visibility,\n",
    "    num_mean_visibility_samples,\n",
    "    mean_wind_speed,\n",
    "    num_mean_wind_speed_samples,\n",
    "    max_sustained_wind_speed,\n",
    "    max_gust_wind_speed,\n",
    "    max_temperature,\n",
    "    max_temperature_explicit,\n",
    "    total_precipitation,\n",
    "    snow_depth,\n",
    "    fog,\n",
    "    rain,\n",
    "    snow,\n",
    "    hail,\n",
    "    thunder,\n",
    "    tornado,\n",
    "    CONCAT(year, '-', month, '-', day) as date\n",
    "FROM \n",
    "    `bigquery-public-data.samples.gsod`\n",
    "WHERE\n",
    "    year BETWEEN 2005 AND 2009 AND\n",
    "    station_number BETWEEN 725300 AND 726300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your instructions read like I am supposed to convert dates to a string in the given format, but I'd rather \n",
    "# workwith pandas native datetime format, as it makes selecting ranges easier.\n",
    "weather_data_raw['date'] = pd.to_datetime(weather_data_raw['date'])\n",
    "weather_data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Task \n",
    "From here want to work with the data from all stations 725300 to 725330 that have information from 2005 till 2009. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data_raw.query('725300 <= station_number <= 725330')\n",
    "weather_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a first analysis of the remaining dataset, clean or drop data depending on how you see appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "date_index = pd.date_range(weather_data['date'].min(), weather_data['date'].max())\n",
    "print(f'We have {len(date_index)} days in the dataset.')\n",
    "for key, group in weather_data.groupby(['station_number']):\n",
    "    print(f'Temperature data for station {key} has {len(group.mean_temp)} entries, e.g. data for {len(date_index) - len(group.mean_temp)} days is missing.')\n",
    "\n",
    "print('Lets see where some of those missing dates are.')\n",
    "\n",
    "def plot_missing_data(column, axis, weather_data):\n",
    "\n",
    "    dates = pd.DataFrame(columns=[station for station in weather_data['station_number'].unique()], index=date_index, data=0)\n",
    "    for key, group in weather_data.groupby(['station_number']):\n",
    "        dates_with_non_null_mean_temp = weather_data[~weather_data[column].isna()][weather_data['station_number']==key]\n",
    "        dates.loc[dates_with_non_null_mean_temp['date'].values, key] = 1\n",
    "    dates.plot(ax=axis)\n",
    "    axis.set_title(f'Days with missing data in {column}')\n",
    "    \n",
    "    \n",
    "fig, axes = plt.subplots(ncols=3, figsize=(18, 5)) \n",
    "\n",
    "plot_missing_data('mean_temp', axes[0], weather_data)\n",
    "plot_missing_data('snow', axes[1], weather_data)\n",
    "plot_missing_data('total_precipitation', axes[2], weather_data)\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random values are distributed across the dataset and not connected periods of time, so an educated guess would be to fill them with sensible defaults.\n",
    "\n",
    "For demperature and pressure time series etc., where the process underlying the measurements is continuous and differentiable, I interpolate the missing values. For boolean variables such as snow, I fill them with with values from a timestep before. This does not skewing the overal distribution but it does increase autocorrelation however which will influence our forecasting results. If I was to spend more time on this, I would check by how much and experiment with different filling methods here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Reindex on station_number and date to get continuous time series.\n",
    "weather_data_reindexed = weather_data.set_index([\"station_number\", \"date\"]).reindex(\n",
    "    pd.MultiIndex.from_product(\n",
    "        [weather_data[\"station_number\"].unique(), date_index],\n",
    "        names=[\"station_number\", \"date\"],\n",
    "    ),\n",
    "    fill_value=float('nan')\n",
    ")\n",
    "\n",
    "# Interpolate values for variables for which the underlying process is assumed to be smooth.\n",
    "weather_data_reindexed['mean_temp'].interpolate(inplace=True)\n",
    "weather_data_reindexed['mean_dew_point'].interpolate(inplace=True)\n",
    "weather_data_reindexed['mean_sealevel_pressure'].interpolate(limit_directionlimit_direction='both', inplace=True)\n",
    "weather_data_reindexed['mean_visibility'].interpolate(inplace=True)\n",
    "weather_data_reindexed['mean_wind_speed'].interpolate(inplace=True)\n",
    "weather_data_reindexed['max_sustained_wind_speed'].interpolate(inplace=True)\n",
    "weather_data_reindexed['max_temperature'].interpolate(inplace=True)\n",
    "weather_data_reindexed['total_precipitation'].interpolate(inplace=True)\n",
    "\n",
    "# Assume that, if there was snow, they'd report it\n",
    "weather_data_reindexed['snow_depth'].fillna(value=0, inplace=True)  \n",
    "\n",
    "# Assume data quality is approximately constant for each station without double checking though.\n",
    "weather_data_reindexed['num_mean_temp_samples'].fillna(method='ffill', inplace=True)\n",
    "weather_data_reindexed['num_mean_dew_point_samples'].fillna(method='ffill', inplace=True)\n",
    "weather_data_reindexed['num_mean_visibility_samples'].fillna(method='ffill', inplace=True)\n",
    "weather_data_reindexed['num_mean_wind_speed_samples'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Fill with previous value, keeps overall distribution approx. same but increases autocorrelation\n",
    "weather_data_reindexed['fog'].fillna(method='ffill', inplace=True)\n",
    "weather_data_reindexed['rain'].fillna(method='ffill', inplace=True)\n",
    "weather_data_reindexed['snow'].fillna(method='ffill', inplace=True)\n",
    "weather_data_reindexed['hail'].fillna(method='ffill', inplace=True)\n",
    "weather_data_reindexed['thunder'].fillna(method='ffill', inplace=True)\n",
    "weather_data_reindexed['tornado'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Drop columns where there are still nans as data quality is too low.\n",
    "weather_data_reindexed.dropna(inplace=True, axis=1)\n",
    "\n",
    "# I suppose, the day of the year could also be a relevant info for the model to guess the likelyhood of snow.\n",
    "weather_data_reindexed = weather_data_reindexed.reset_index()\n",
    "weather_data_reindexed['day_of_year'] = weather_data_reindexed['date'].apply(lambda col: pd.Period(col, freq='D').dayofyear)\n",
    "\n",
    "for column in ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']:\n",
    "    weather_data_reindexed[column] = weather_data_reindexed[column].astype(int)\n",
    "\n",
    "\n",
    "weather_data_cleaned = weather_data_reindexed\n",
    "weather_data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Task\n",
    "Now it is time to split the data, into a training, evaluation and test set. As a reminder, the date we are trying to predict snow fall for is the following, and hence should constitute your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "today_12_years_ago = str(datetime.today()- timedelta(days=12*365)).split(' ')[0]\n",
    "a_week_ago_12_years_ago = str(datetime.today()- timedelta(days=12*365) - timedelta(days=8)).split(' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = weather_data_cleaned[weather_data_cleaned['date']<'2009-09-01']\n",
    "evaluation_set = weather_data_cleaned[weather_data_cleaned['date']>='2009-09-01']\n",
    "test_set = weather_data_cleaned[weather_data_cleaned['date']<=today_12_years_ago][weather_data_cleaned['date']>=a_week_ago_12_years_ago]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_training_data(training_days: int, dataframe: pd.DataFrame):\n",
    "    tmp = dataframe.sort_values(['date', 'station_number'])\n",
    "    tmp_no_dates = tmp.drop(['date', 'station_number'], axis=1)\n",
    "    date_range = pd.date_range(tmp['date'].min(), tmp['date'].max())\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    for i in tqdm(range(len(date_range)-(training_days+1))):\n",
    "        days = date_range[i:i+training_days+1]\n",
    "        x = tmp_no_dates[dataframe['date'].isin(days[:training_days])].values\n",
    "        y = tmp_no_dates['snow'][dataframe['date'] == days[-1]].values\n",
    "        x_values.append([item for sublist in x for item in sublist])\n",
    "        y_values.append(list(y))\n",
    "    \n",
    "    return tf.keras.utils.normalize(x_values, axis=1), tf.keras.utils.normalize(y_values, axis=1)\n",
    "\n",
    "forecasting_days = 3\n",
    "\n",
    "x_train, y_train = generate_training_data(forecasting_days,  training_set)\n",
    "x_eval, y_eval = generate_training_data(forecasting_days, evaluation_set)\n",
    "x_test, y_test = generate_training_data(forecasting_days, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "If you made it up to here all by yourself, you can use your prepared dataset to train an Algorithm of your choice to forecast whether it will snow on the following date for each station in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "str(datetime.datetime.today()- datetime.timedelta(days=12*365)).split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are allowed to use any library you are comfortable with such as sklearn, tensorflow keras etc. \n",
    "If you did not manage to finish part one feel free to use the data provided in 'coding_challenge.csv' Note that this data does not represent a solution to Part 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "modelNN = tf.keras.models.Sequential()\n",
    "\n",
    "# Flatten inputs for multiple days\n",
    "modelNN.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Input Layers - relu, because it seems to be a good default\n",
    "modelNN.add(tf.keras.layers.Dense(len(x_train[0]), activation=tf.nn.relu))\n",
    "\n",
    "# Hidden Layer\n",
    "modelNN.add(tf.keras.layers.Dense(2*len(y_train[0]), activation=tf.nn.relu))\n",
    "modelNN.add(tf.keras.layers.Dense(3*len(y_train[0])//2, activation=tf.nn.relu))\n",
    "\n",
    "# Output Layer - sigmoid for binary classification\n",
    "modelNN.add(tf.keras.layers.Dense(len(y_train[0]), activation=tf.nn.relu))\n",
    "\n",
    "\n",
    "modelNN.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, decay=1e-6),\n",
    "    loss='mse',\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit --port=8080\n",
    "\n",
    "log_dir = \"logs/fit/\" + 'NN' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "modelNN.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_eval, y_eval),\n",
    "    epochs=1000,\n",
    "    verbose=0,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converging slowly but looking at the evaluation sample shows that its overfitting.\n",
    "\n",
    "Try a recurring neural network. Maybe that works better for time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_squences(training_days: int, dataframe: pd.DataFrame):\n",
    "    tmp = dataframe.sort_values(['date', 'station_number'])\n",
    "    tmp_no_dates = tmp.drop(['date', 'station_number'], axis=1)\n",
    "    date_range = pd.date_range(tmp['date'].min(), tmp['date'].max())\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    for i in tqdm(range(len(date_range)-(training_days+1))):\n",
    "        days = date_range[i:i+training_days+1]\n",
    "        x = tmp_no_dates[dataframe['date'].isin(days[:training_days])].values\n",
    "        y = tmp_no_dates['snow'][dataframe['date'] == days[-1]].values\n",
    "        \n",
    "        # don't flatten data for several days but keep it as sqeuences for LSTM\n",
    "        x_values.append(x)\n",
    "        y_values.append(y)\n",
    "    \n",
    "    return tf.keras.utils.normalize(x_values, axis=1), tf.keras.utils.normalize(y_values, axis=1)\n",
    "\n",
    "forecasting_days = 7\n",
    "\n",
    "x_train, y_train = generate_training_data_squences(forecasting_days,  training_set)\n",
    "x_eval, y_eval = generate_training_data_squences(forecasting_days, evaluation_set)\n",
    "x_test, y_test = generate_training_data_squences(forecasting_days, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, RepeatVector, TimeDistributed\n",
    "\n",
    "modelRNN = Sequential()\n",
    "modelRNN.add(\n",
    "    LSTM(128, input_shape=(x_train.shape[1:]), activation=\"relu\", return_sequences=True)\n",
    ")\n",
    "modelRNN.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "modelRNN.add(LSTM(50, activation=\"relu\"))\n",
    "\n",
    "modelRNN.add(Dense(10, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "modelRNN.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "log_dir = \"logs/fit/\" + 'RNN' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "modelRNN.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(x_eval, y_eval),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
